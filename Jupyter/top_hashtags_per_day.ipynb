{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract top 100 hashtags per day from Ukraine Conflict Tweet Dataset\n",
    "* need to first combine or split any csvs that are in multiple parts\n",
    "    * can try to do it automatically using regex\n",
    "* all csv files have the same three columns: tweetid, hashtags, language\n",
    "    * only keep 'en' language tweets\n",
    "    * only keep tweets with at least one hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import datetime\n",
    "import calendar\n",
    "import pandas as pd\n",
    "\n",
    "PD_READ_CSV_LOW_MEMORY_FLAG = False # Set to True if your computer has low memory\n",
    "CSV_WRITE_UTF_ENCODING = 'utf16' #'utf16' #'utf-8' causes loss/corruption of certain characters\n",
    "HASHTAG_FREQUENCY_THRESHOLD = 100 # Minimum number of times a hashtag must be used to be included in the top hashtags\n",
    "NUMBER_TOP_HASHTAGS = 100 # Number of top hashtags to be included in the report/csv\n",
    "\n",
    "DATA_FOLDER_PATH = '..\\\\data\\\\'\n",
    "\n",
    "BACKUP_RAW_CSV_FILES_FOLDER_PATH = DATA_FOLDER_PATH + \"archive_backup\\\\\" # up Jupyter\\ Folder, into data\\archive_backup\\\n",
    "RAW_CSV_FILES_FOLDER_PATH = DATA_FOLDER_PATH + \"archive\\\\\" # up Jupyter\\ Folder, into data\\archive\\\n",
    "TOP_HASHTAGS_PER_DAY_FOLDER_PATH =  DATA_FOLDER_PATH + \"top_hashtags_per_day\\\\\" # up Jupyter\\ Folder, into data\\top_hashtags_per_day\\\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files/Data Setup\n",
    "* if you want to start from scratch, remove everything in the `data\\archive` folder and the \n",
    "    * the code will automatically copy the raw csvs from the `data\\archive_backup` folder to the `data\\archive` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folder: ..\\data\\archive\\\n",
      "Created folder: ..\\data\\top_hashtags_per_day\\\n"
     ]
    }
   ],
   "source": [
    "# Folder creation\n",
    "\n",
    "if not os.path.exists(BACKUP_RAW_CSV_FILES_FOLDER_PATH):\n",
    "    print(\"Unable to find backup folder: \" + BACKUP_RAW_CSV_FILES_FOLDER_PATH)\n",
    "    os.sys.exit(1)\n",
    "\n",
    "if not os.path.exists(RAW_CSV_FILES_FOLDER_PATH):\n",
    "    os.makedirs(RAW_CSV_FILES_FOLDER_PATH)\n",
    "    print(\"Created folder: \" + RAW_CSV_FILES_FOLDER_PATH)\n",
    "    \n",
    "if not os.path.exists(TOP_HASHTAGS_PER_DAY_FOLDER_PATH):\n",
    "    os.makedirs(TOP_HASHTAGS_PER_DAY_FOLDER_PATH)\n",
    "    print(\"Created folder: \" + TOP_HASHTAGS_PER_DAY_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment the opening \"\"\" in order to delete all files in both the `data\\archive` folder and the `data\\top_hashtags_per_day` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# https://stackoverflow.com/a/12526809\\ndef delete_folder_contents(dirpath):\\n    for filename in os.listdir(dirpath):\\n        filepath = os.path.join(dirpath, filename)\\n        try:\\n            shutil.rmtree(filepath)\\n        except OSError:\\n            os.remove(filepath)\\n\\ndelete_folder_contents(RAW_CSV_FILES_FOLDER_PATH)\\ndelete_folder_contents(TOP_HASHTAGS_PER_DAY_FOLDER_PATH)\\n           \\n#'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ONLY UNCOMMENT AND RUN THIS IF YOU WANT TO DELETE the contents of the archive\\ and top_hashtags_per_day\\ folder\n",
    "\n",
    "\"\"\"\n",
    "# https://stackoverflow.com/a/12526809\n",
    "def delete_folder_contents(dirpath):\n",
    "    for filename in os.listdir(dirpath):\n",
    "        filepath = os.path.join(dirpath, filename)\n",
    "        try:\n",
    "            shutil.rmtree(filepath)\n",
    "        except OSError:\n",
    "            os.remove(filepath)\n",
    "\n",
    "delete_folder_contents(RAW_CSV_FILES_FOLDER_PATH)\n",
    "delete_folder_contents(TOP_HASHTAGS_PER_DAY_FOLDER_PATH)\n",
    "           \n",
    "#\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy raw csvs from `data\\archive_backup` folder to `data\\archive` folder (if not already there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No raw csv files found in: ..\\data\\archive\\\n",
      "Copying files from: ..\\data\\archive_backup\\\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PERFORM_PROCESSING = False\n",
    "\n",
    "raw_csv_backup_filenames = [f for f in os.listdir(BACKUP_RAW_CSV_FILES_FOLDER_PATH) if f.endswith('.csv')]\n",
    "raw_csv_backup_filepaths = [os.path.join(BACKUP_RAW_CSV_FILES_FOLDER_PATH, f) for f in raw_csv_backup_filenames]\n",
    "raw_csv_filepaths = [os.path.join(RAW_CSV_FILES_FOLDER_PATH, f) for f in raw_csv_backup_filenames]\n",
    "\n",
    "# if both folders are empty, copy raw csvs from backup archive folder to raw archive folder\n",
    "if len(os.listdir(RAW_CSV_FILES_FOLDER_PATH)) == 0 and len(os.listdir(TOP_HASHTAGS_PER_DAY_FOLDER_PATH)) == 0:\n",
    "    print(\"No raw csv files found in: \" + RAW_CSV_FILES_FOLDER_PATH + \"\\nCopying files from: \" + BACKUP_RAW_CSV_FILES_FOLDER_PATH)\n",
    "\n",
    "    for backup_filepath, filepath in zip(raw_csv_backup_filepaths, raw_csv_filepaths):\n",
    "        shutil.copy(backup_filepath, filepath)\n",
    "        \n",
    "    PERFORM_PROCESSING = True\n",
    "    \n",
    "elif len(os.listdir(RAW_CSV_FILES_FOLDER_PATH)) > 0 and len(os.listdir(TOP_HASHTAGS_PER_DAY_FOLDER_PATH)) > 0:\n",
    "    PERFORM_PROCESSING = False\n",
    "    \n",
    "elif len(os.listdir(RAW_CSV_FILES_FOLDER_PATH)) > 0 and len(os.listdir(TOP_HASHTAGS_PER_DAY_FOLDER_PATH)) == 0:\n",
    "    PERFORM_PROCESSING = True\n",
    "\n",
    "print(PERFORM_PROCESSING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract all Date-Filenames pairings from the `data\\archive` folder using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{datetime.date(2022, 4, 1): ['0401_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 2): ['0402_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 3): ['0403_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 4): ['0404_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 5): ['0405_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 6): ['0406_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 7): ['0407_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 8): ['0408_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 9): ['0409_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 10): ['0410_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 11): ['0411_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 12): ['0412_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 13): ['0413_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 14): ['0414_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 15): ['0415_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 16): ['0416_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 17): ['0417_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 18): ['0418_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 19): ['0419_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 20): ['0420_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 21): ['0421_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 22): ['0422_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 23): ['0423_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 24): ['0424_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 25): ['0425_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 26): ['0426_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 27): ['0427_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 28): ['0428_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 29): ['0429_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 4, 30): ['0430_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 1): ['0501_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 2): ['0502_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 3): ['0503_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 4): ['0504_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 5): ['0505_to_0507_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 6): ['0505_to_0507_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 7): ['0505_to_0507_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 8): ['0508_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 9): ['0509_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 10): ['0510_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 11): ['0511_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 12): ['0512_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 13): ['0513_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 14): ['0514_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 15): ['0515_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 16): ['0516_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 17): ['0517_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 18): ['0518_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 19): ['0519_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 20): ['0520_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 21): ['0521_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 22): ['0522_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 23): ['0523_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 24): ['0524_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 25): ['0525_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 26): ['0526_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 27): ['0527_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 28): ['0528_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 29): ['0529_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 30): ['0530_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 5, 31): ['0531_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 1): ['0601_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 2): ['0602_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 3): ['0603_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 4): ['0604_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 5): ['0605_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 6): ['0606_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 7): ['0607_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 8): ['0608_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 9): ['0609_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 10): ['0610_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 11): ['0611_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 12): ['0612_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 2, 27): ['UkraineCombinedTweetsDeduped20220227-131611.csv', 'UkraineCombinedTweetsDeduped_FEB27.csv'], datetime.date(2022, 2, 28): ['UkraineCombinedTweetsDeduped_FEB28_part1.csv', 'UkraineCombinedTweetsDeduped_FEB28_part2.csv'], datetime.date(2022, 3, 1): ['UkraineCombinedTweetsDeduped_MAR01.csv'], datetime.date(2022, 3, 2): ['UkraineCombinedTweetsDeduped_MAR02.csv'], datetime.date(2022, 3, 3): ['UkraineCombinedTweetsDeduped_MAR03.csv'], datetime.date(2022, 3, 4): ['UkraineCombinedTweetsDeduped_MAR04.csv'], datetime.date(2022, 3, 5): ['UkraineCombinedTweetsDeduped_MAR05.csv'], datetime.date(2022, 3, 6): ['UkraineCombinedTweetsDeduped_MAR06.csv'], datetime.date(2022, 3, 7): ['UkraineCombinedTweetsDeduped_MAR07.csv'], datetime.date(2022, 3, 8): ['UkraineCombinedTweetsDeduped_MAR08.csv'], datetime.date(2022, 3, 9): ['UkraineCombinedTweetsDeduped_MAR09.csv'], datetime.date(2022, 3, 10): ['UkraineCombinedTweetsDeduped_MAR10.csv'], datetime.date(2022, 3, 11): ['UkraineCombinedTweetsDeduped_MAR11.csv'], datetime.date(2022, 3, 12): ['UkraineCombinedTweetsDeduped_MAR12.csv'], datetime.date(2022, 3, 13): ['UkraineCombinedTweetsDeduped_MAR13.csv'], datetime.date(2022, 3, 14): ['UkraineCombinedTweetsDeduped_MAR14.csv'], datetime.date(2022, 3, 15): ['UkraineCombinedTweetsDeduped_MAR15.csv'], datetime.date(2022, 3, 16): ['UkraineCombinedTweetsDeduped_MAR16.csv'], datetime.date(2022, 3, 17): ['UkraineCombinedTweetsDeduped_MAR17.csv'], datetime.date(2022, 3, 18): ['UkraineCombinedTweetsDeduped_MAR18.csv'], datetime.date(2022, 3, 19): ['UkraineCombinedTweetsDeduped_MAR19.csv'], datetime.date(2022, 3, 20): ['UkraineCombinedTweetsDeduped_MAR20.csv'], datetime.date(2022, 3, 21): ['UkraineCombinedTweetsDeduped_MAR21.csv'], datetime.date(2022, 3, 22): ['UkraineCombinedTweetsDeduped_MAR22.csv'], datetime.date(2022, 3, 23): ['UkraineCombinedTweetsDeduped_MAR23.csv'], datetime.date(2022, 3, 24): ['UkraineCombinedTweetsDeduped_MAR24.csv'], datetime.date(2022, 3, 25): ['UkraineCombinedTweetsDeduped_MAR25.csv'], datetime.date(2022, 3, 26): ['UkraineCombinedTweetsDeduped_MAR26.csv'], datetime.date(2022, 3, 27): ['UkraineCombinedTweetsDeduped_MAR27_to_28.csv'], datetime.date(2022, 3, 28): ['UkraineCombinedTweetsDeduped_MAR27_to_28.csv'], datetime.date(2022, 3, 29): ['UkraineCombinedTweetsDeduped_MAR29.csv'], datetime.date(2022, 3, 30): ['UkraineCombinedTweetsDeduped_MAR30_REAL.csv'], datetime.date(2022, 3, 31): ['UkraineCombinedTweetsDeduped_MAR31.csv']}\n"
     ]
    }
   ],
   "source": [
    "# this is a guard clause to prevent the script from running if the output files are already in the archive\\ folder\n",
    "if not PERFORM_PROCESSING: \n",
    "    os.sys.exit(1) # exit with error code 1 if we don't need to perform processing   \n",
    "\n",
    "\n",
    "# extract the date from the file name, to aid in joining/spliting the parts automatically\n",
    "\"\"\" CSVs are in one of these formats:\n",
    "    UkraineCombinedTweetsDeduped20220227-131611.csv\n",
    "    UkraineCombinedTweetsDeduped_FEB27.csv\n",
    "    UkraineCombinedTweetsDeduped_FEB28_part1.csv\n",
    "    UkraineCombinedTweetsDeduped_MAR27_to_28.csv\n",
    "    UkraineCombinedTweetsDeduped_MAR30_REAL.csv\n",
    "    0401_0UkraineCombinedTweetsDeduped\n",
    "    0505_to_0507_UkraineCombinedTweetsDeduped.csv\n",
    "\"\"\" \n",
    "# regex for each of these cases, extract the month and day\n",
    "\n",
    "UKRAINE_BLURB=\"UkraineCombinedTweetsDeduped\"\n",
    "FIRST_CSV_HOUR_MIN_SEC = '131611'\n",
    "DATA_YEAR = 2022\n",
    "\n",
    "month_abbr_to_int = dict((v,k) for v,k in zip([m.lower() for m in calendar.month_abbr[1:]], range(1, 13)))\n",
    "\n",
    "regex1 = re.compile(r'^' + UKRAINE_BLURB + r'(?P<year>\\d{4})(?P<month>\\d{2})(?P<day>\\d{2})-' + FIRST_CSV_HOUR_MIN_SEC + '.csv$')\n",
    "regex2 = re.compile(r'^' + UKRAINE_BLURB + r'_(?P<month_abbr>\\w{3})(?P<day>\\d{2})\\.csv$')\n",
    "regex3 = re.compile(r'^' + UKRAINE_BLURB + r'_(?P<month_abbr>\\w{3})(?P<day>\\d{2})_part\\d\\.csv$')\n",
    "regex4 = re.compile(r'^' + UKRAINE_BLURB + r'_(?P<month_abbr1>\\w{3})(?P<day1>\\d{2})_to_(?:\\w{3})?(?P<day2>\\d{2})\\.csv$')\n",
    "regex4_2 = re.compile(r'^' + UKRAINE_BLURB + r'_(?P<month_abbr1>\\w{3})(?P<day1>\\d{2})_to_(?P<month_abbr2>\\w{3})(?P<day2>\\d{2})\\.csv$')\n",
    "regex5 = re.compile(r'^' + UKRAINE_BLURB + r'_(?P<month_abbr>\\w{3})(?P<day>\\d{2})_REAL\\.csv$')\n",
    "regex6 = re.compile(r'^(?P<month>\\d{2})(?P<day>\\d{2})_' + UKRAINE_BLURB + r'.csv')\n",
    "regex7 = re.compile(r'^(?P<month1>\\d{2})(?P<day1>\\d{2})_to_(?:\\d{2})?(?P<day2>\\d{2})_' + UKRAINE_BLURB + r'.csv')\n",
    "regex7_2 = re.compile(r'^(?P<month1>\\d{2})(?P<day1>\\d{2})_to_(?P<month2>\\d{2})(?P<day2>\\d{2})_' + UKRAINE_BLURB + r'.csv')\n",
    "\n",
    "def extract_date_range_from_filename(filename):\n",
    "    m1 = re.match(regex1, filename)\n",
    "    m2 = re.match(regex2, filename)\n",
    "    m3 = re.match(regex3, filename)\n",
    "    m4 = re.match(regex4, filename)\n",
    "    m5 = re.match(regex5, filename)\n",
    "    m6 = re.match(regex6, filename)\n",
    "    m7 = re.match(regex7, filename)\n",
    "    \n",
    "    if m1:\n",
    "        return [datetime.date(year=int(m1.group('year')), month=int(m1.group('month')), day=int(m1.group('day')))]\n",
    "    if m2:\n",
    "        return [datetime.date(year=DATA_YEAR, month=month_abbr_to_int[m2.group('month_abbr').lower()], day=int(m2.group('day')))]\n",
    "    if m3:\n",
    "        return [datetime.date(year=DATA_YEAR, month=month_abbr_to_int[m3.group('month_abbr').lower()], day=int(m3.group('day')))]\n",
    "    if m4:\n",
    "        sdate = datetime.date(year=DATA_YEAR, month=month_abbr_to_int[m4.group('month_abbr1').lower()], day=int(m4.group('day1')))\n",
    "        \n",
    "        m4_2 = re.match(regex4_2, filename) # this is the case where the 2nd month is not specified\n",
    "        if m4_2:\n",
    "            edate = datetime.date(year=DATA_YEAR, month=month_abbr_to_int[m4_2.group('month_abbr2').lower()], day=int(m4_2.group('day2')))\n",
    "        else:\n",
    "            edate = datetime.date(year=DATA_YEAR, month=month_abbr_to_int[m4.group('month_abbr1').lower()], day=int(m4.group('day2')))\n",
    "       \n",
    "        # https://stackoverflow.com/a/66595046\n",
    "        return [sdate+datetime.timedelta(days=x) for x in range(0,(edate-sdate).days+1)]\n",
    "    if m5:\n",
    "        return [datetime.date(year=DATA_YEAR, month=month_abbr_to_int[m5.group('month_abbr').lower()], day=int(m5.group('day')))]\n",
    "    if m6:\n",
    "        return [datetime.date(year=DATA_YEAR, month=int(m6.group('month')), day=int(m6.group('day')))]\n",
    "    if m7:\n",
    "        sdate = datetime.date(year=DATA_YEAR, month=int(m7.group('month1')), day=int(m7.group('day1')))\n",
    "        \n",
    "        m7_2 = re.match(regex7_2, filename) # this is for the case where the second month is not specified\n",
    "        \n",
    "        if m7_2:\n",
    "            edate = datetime.date(year=DATA_YEAR, month=int(m7_2.group('month2')), day=int(m7.group('day2'))) \n",
    "        else:\n",
    "            edate = datetime.date(year=DATA_YEAR, month=int(m7.group('month1')), day=int(m7.group('day2'))) \n",
    "            \n",
    "        # https://stackoverflow.com/a/66595046\n",
    "        return [sdate+datetime.timedelta(days=x) for x in range(0,(edate-sdate).days+1)]\n",
    "\n",
    "            \n",
    "############### MAIN SCRIPT STARTS HERE ###################\n",
    "\n",
    "# Get dictionary of dates to filenames\n",
    "date_to_csv_filenames = {}\n",
    "\n",
    "for filename in os.listdir(RAW_CSV_FILES_FOLDER_PATH):\n",
    "    if filename.endswith('.csv'):\n",
    "        date_range = extract_date_range_from_filename(filename)\n",
    "        if date_range is not None:\n",
    "            for date in date_range:\n",
    "                if date not in date_to_csv_filenames:\n",
    "                    date_to_csv_filenames[date] = []\n",
    "                date_to_csv_filenames[date].append(filename)\n",
    "\n",
    "print(date_to_csv_filenames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine/Split and Rename raw csv files as needed across each Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliting filename: 0505_to_0507_UkraineCombinedTweetsDeduped.csv over [datetime.date(2022, 5, 5), datetime.date(2022, 5, 6), datetime.date(2022, 5, 7)]\n",
      "Removing file: 0505_to_0507_UkraineCombinedTweetsDeduped.csv\n",
      "Combining files ['UkraineCombinedTweetsDeduped20220227-131611.csv', 'UkraineCombinedTweetsDeduped_FEB27.csv'] for date: 2022-02-27\n",
      "Removing file: UkraineCombinedTweetsDeduped20220227-131611.csv\n",
      "Removing file: UkraineCombinedTweetsDeduped_FEB27.csv\n",
      "Combining files ['UkraineCombinedTweetsDeduped_FEB28_part1.csv', 'UkraineCombinedTweetsDeduped_FEB28_part2.csv'] for date: 2022-02-28\n",
      "Removing file: UkraineCombinedTweetsDeduped_FEB28_part1.csv\n",
      "Removing file: UkraineCombinedTweetsDeduped_FEB28_part2.csv\n",
      "Spliting filename: UkraineCombinedTweetsDeduped_MAR27_to_28.csv over [datetime.date(2022, 3, 27), datetime.date(2022, 3, 28)]\n",
      "Removing file: UkraineCombinedTweetsDeduped_MAR27_to_28.csv\n",
      "Renaming files in ..\\data\\archive\\ to match format: 2022_07_08.csv\n"
     ]
    }
   ],
   "source": [
    "if len(os.listdir(RAW_CSV_FILES_FOLDER_PATH)) > 0 and len(os.listdir(TOP_HASHTAGS_PER_DAY_FOLDER_PATH)) == 0:\n",
    "    PERFORM_PROCESSING = True\n",
    "\n",
    "# this is a guard clause to prevent the script from running if the output files are already in the archive\\ folder\n",
    "if not PERFORM_PROCESSING: \n",
    "    os.sys.exit(1) # exit with error code 1 if we don't need to perform processing   \n",
    "\n",
    "def get_new_filename_from_date(date):\n",
    "    return f'{date.year:04d}_{date.month:02d}_{date.day:02d}.csv'\n",
    "\n",
    "def combine_files_and_remove_predecessors(date, old_filenames):\n",
    "    print('Combining files ' + str(old_filenames) + ' for date: ' + str(date))\n",
    "    \n",
    "    dfs_to_combine = []\n",
    "    \n",
    "    for old_filename in old_filenames:\n",
    "        dfs_to_combine.append(pd.read_csv(RAW_CSV_FILES_FOLDER_PATH + '/' + old_filename, low_memory=PD_READ_CSV_LOW_MEMORY_FLAG))\n",
    "    \n",
    "    df_combined = pd.concat(dfs_to_combine)\n",
    "        \n",
    "    df_combined.to_csv(RAW_CSV_FILES_FOLDER_PATH + '/' + get_new_filename_from_date(date), index=False)\n",
    "    \n",
    "    # remove old files\n",
    "    for old_filename in old_filenames:\n",
    "        filepath = os.path.join(RAW_CSV_FILES_FOLDER_PATH, old_filename)\n",
    "        print(\"Removing file: \" + old_filename)\n",
    "        try:\n",
    "            shutil.rmtree(filepath)\n",
    "        except OSError:\n",
    "            os.remove(filepath)\n",
    "\n",
    "\n",
    "# from https://ws-dl.blogspot.com/2019/08/2019-08-03-tweetedat-finding-tweet.html\n",
    "def get_date_from_tweetid(tid):\n",
    "    offset = 1288834974657 # UTC offset in milliseconds\n",
    "    tstamp = (tid >> 22) + offset\n",
    "    return datetime.datetime.utcfromtimestamp(tstamp/1000).date()\n",
    "\n",
    "\n",
    "def split_file_over_necessary_dates(filename, dates):\n",
    "    print(\"Spliting filename: \" + filename + \" over \" + str(dates)) \n",
    "    \n",
    "    df_to_split = pd.read_csv(RAW_CSV_FILES_FOLDER_PATH + '/' + filename, low_memory=PD_READ_CSV_LOW_MEMORY_FLAG)\n",
    "    df_to_split['date'] = df_to_split['tweetid'].map(get_date_from_tweetid)\n",
    "    \n",
    "    # Create a new dataframe for each date, by filtering the original dataframe by the date\n",
    "    for date in dates:\n",
    "        df_date = df_to_split[pd.to_datetime(df_to_split['date']).dt.date == date]\n",
    "        \n",
    "        df_date.to_csv(RAW_CSV_FILES_FOLDER_PATH + '/' + get_new_filename_from_date(date), index=False)\n",
    "        \n",
    "    # Remove the original file\n",
    "    filepath = os.path.join(RAW_CSV_FILES_FOLDER_PATH, filename)\n",
    "    print(\"Removing file: \" + filename)\n",
    "    try:\n",
    "        shutil.rmtree(filepath)\n",
    "    except OSError:\n",
    "        os.remove(filepath)\n",
    "   \n",
    "\n",
    "############### MAIN SCRIPT STARTS HERE ###################\n",
    "\n",
    "last_date = None\n",
    "last_filename = \"\"\n",
    "current_filename = \"\"\n",
    "num_dates_emcompassed_by_single_file = 1\n",
    "\n",
    "for date in date_to_csv_filenames:\n",
    "    current_filename = date_to_csv_filenames[date][0]\n",
    "    \n",
    "    if len(date_to_csv_filenames[date]) > 1:\n",
    "        \n",
    "        combine_files_and_remove_predecessors(date, date_to_csv_filenames[date])\n",
    "        \n",
    "    elif current_filename != last_filename and num_dates_emcompassed_by_single_file > 1:\n",
    "        \n",
    "        dates = list(reversed([last_date-datetime.timedelta(days=x) for x in range(0,num_dates_emcompassed_by_single_file)]))\n",
    "        split_file_over_necessary_dates(last_filename, dates)\n",
    "        \n",
    "        num_dates_emcompassed_by_single_file = 1\n",
    "\n",
    "    if last_filename == current_filename:\n",
    "        num_dates_emcompassed_by_single_file += 1    \n",
    "       \n",
    "    last_date = date \n",
    "    last_filename = current_filename\n",
    "\n",
    "# Case where the last date is part of a file that needs to be split\n",
    "if current_filename != last_filename and num_dates_emcompassed_by_single_file > 1:\n",
    "    dates = list(reversed([last_date-datetime.timedelta(days=x) for x in range(0,num_dates_emcompassed_by_single_file)]))\n",
    "    split_file_over_necessary_dates(last_filename, dates)\n",
    "    \n",
    "    \n",
    "# Rename all files to the format indicated by get_new_filename_from_date(date)\n",
    "print(f'Renaming files in {RAW_CSV_FILES_FOLDER_PATH} to match format: {get_new_filename_from_date(datetime.datetime.now().date())}') \n",
    "\n",
    "# matches filenames of the form: 2019_08_03.csv, as defined by the get_new_filename_from_date(date) function\n",
    "regex8 = re.compile(r'^(?P<year>\\d{4})_(?P<month>\\d{2})_(?P<day>\\d{2}).csv$') \n",
    "\n",
    "for filename in os.listdir(RAW_CSV_FILES_FOLDER_PATH):\n",
    "    if not re.match(regex8, filename): # if the filename does not match the regex, rename it\n",
    "        os.rename(RAW_CSV_FILES_FOLDER_PATH + '/' + filename,\n",
    "                  RAW_CSV_FILES_FOLDER_PATH + '/' + get_new_filename_from_date(extract_date_range_from_filename(filename)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Processing to the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote top 100 hashtags to file: 2022_02_27.csv\n",
      "Wrote top 100 hashtags to file: 2022_02_28.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_01.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_02.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_03.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_04.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_05.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_06.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_07.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_08.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_09.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_10.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_11.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_12.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_13.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_14.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_15.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_16.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_17.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_18.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_19.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_20.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_21.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_22.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_23.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_24.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_25.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_26.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_27.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_28.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_29.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_30.csv\n",
      "Wrote top 100 hashtags to file: 2022_03_31.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_01.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_02.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_03.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_04.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_05.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_06.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_07.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_08.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_09.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_10.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_11.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_12.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_13.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_14.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_15.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_16.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_17.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_18.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_19.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_20.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_21.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_22.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_23.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_24.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_25.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_26.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_27.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_28.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_29.csv\n",
      "Wrote top 100 hashtags to file: 2022_04_30.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_01.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_02.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_03.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_04.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_05.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_06.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_07.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_08.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_09.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_10.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_11.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_12.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_13.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_14.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_15.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_16.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_17.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_18.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_19.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_20.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_21.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_22.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_23.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_24.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_25.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_26.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_27.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_28.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_29.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_30.csv\n",
      "Wrote top 100 hashtags to file: 2022_05_31.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_01.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_02.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_03.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_04.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_05.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_06.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_07.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_08.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_09.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_10.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_11.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_12.csv\n"
     ]
    }
   ],
   "source": [
    "# this is a guard clause to prevent the script from running if the output files are already in the archive\\ folder\n",
    "if not PERFORM_PROCESSING: \n",
    "    os.sys.exit(1) # exit with error code 1 if we don't need to perform processing   \n",
    "\n",
    "def simplify_hashtags(htag_json):\n",
    "    htag_json = htag_json.replace('\\'','\\\"')\n",
    "    return [ht['text'].lower() for ht in json.loads(htag_json)]\n",
    "    \n",
    "    \n",
    "# from https://ws-dl.blogspot.com/2019/08/2019-08-03-tweetedat-finding-tweet.html\n",
    "def get_tweet_timestamp(tid):\n",
    "    offset = 1288834974657 # UTC offset in milliseconds\n",
    "    tstamp = (tid >> 22) + offset\n",
    "    return datetime.datetime.utcfromtimestamp(tstamp/1000)\n",
    "\n",
    "\n",
    "def clean_ukraine_conflict_twitter_dataframe(df):\n",
    "\n",
    "    # only keep English tweets with non-empty hashtags hashtags\n",
    "    df = df.loc[(df['language'].map(lambda d: d == 'en')) & (df['hashtags'].map(lambda d: d != '[]')), ['tweetid','hashtags']] \n",
    "\n",
    "    df['hashtags'] = df['hashtags'].map(simplify_hashtags)\n",
    "\n",
    "    df['tweet_timestamp'] = df['tweetid'].map(get_tweet_timestamp)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_top_n_hashtags_to_freq_dict(df, num_top_hashtags=NUMBER_TOP_HASHTAGS):\n",
    "\n",
    "    hashtag_to_freq = {}\n",
    "\n",
    "    # count hashtags\n",
    "    for ht_list in df['hashtags'].to_dict().values(): # key is index, value is list of hashtags\n",
    "        for ht in ht_list:\n",
    "            if ht not in hashtag_to_freq:\n",
    "                hashtag_to_freq[ht] = 1\n",
    "            else:\n",
    "                hashtag_to_freq[ht] += 1\n",
    "                \n",
    "    # remove hashtags below threshold (for faster sorting, smaller filesizes)\n",
    "    for ht in list(hashtag_to_freq.keys()):\n",
    "        if hashtag_to_freq[ht] < HASHTAG_FREQUENCY_THRESHOLD:\n",
    "            del hashtag_to_freq[ht]\n",
    "        \n",
    "    # sort hashtags by frequency\n",
    "    return dict(sorted(hashtag_to_freq.items(), key=lambda item: item[1], reverse=True)[0:num_top_hashtags])\n",
    "\n",
    "\n",
    "def write_top_n_hashtags_to_csv(hashtag_to_freq_dict, filename, field_names):\n",
    "    with open(os.path.join(TOP_HASHTAGS_PER_DAY_FOLDER_PATH, filename), 'w', encoding=CSV_WRITE_UTF_ENCODING) as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=field_names, lineterminator = '\\n')\n",
    "        writer.writeheader()\n",
    "        \n",
    "        list_of_fieldname_dicts = []\n",
    "        for ht, freq in hashtag_to_freq_dict.items():\n",
    "            list_of_fieldname_dicts.append({'hashtag': ht, 'frequency': freq})\n",
    "            \n",
    "        writer.writerows(list_of_fieldname_dicts)\n",
    "\n",
    "\n",
    "############### MAIN SCRIPT STARTS HERE ###################\n",
    "\n",
    "field_names = ['hashtag','frequency']\n",
    "\n",
    "# these files should be properly organized by date now\n",
    "for filename in os.listdir(RAW_CSV_FILES_FOLDER_PATH):\n",
    "    df = pd.read_csv(os.path.join(RAW_CSV_FILES_FOLDER_PATH, filename), low_memory=PD_READ_CSV_LOW_MEMORY_FLAG)\n",
    "    df = clean_ukraine_conflict_twitter_dataframe(df)\n",
    "    hashtag_to_freq_dict = get_top_n_hashtags_to_freq_dict(df, NUMBER_TOP_HASHTAGS)\n",
    "    \n",
    "    \n",
    "    write_top_n_hashtags_to_csv(hashtag_to_freq_dict, filename, field_names)\n",
    "    print(\"Wrote top \" + str(NUMBER_TOP_HASHTAGS) + \" hashtags to file: \" + filename)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4a48f9b014948d177db8b96258f87377bfd9aeb2f0be1739b178ffdde29ce0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
