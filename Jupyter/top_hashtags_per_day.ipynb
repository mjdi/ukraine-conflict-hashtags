{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract top 100 hashtags per day from Ukraine Conflict Tweet Dataset\n",
    "* need to first combine or split any csvs that are in multiple parts\n",
    "    * can try to do it automatically using regex\n",
    "* all csv files have the same three columns: tweetid, hashtags, language\n",
    "    * only keep 'en' language tweets\n",
    "    * only keep tweets with at least one hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import datetime\n",
    "import calendar\n",
    "import pandas as pd\n",
    "\n",
    "PD_READ_CSV_LOW_MEMORY_FLAG = False # Set to True if your computer has low memory\n",
    "CSV_WRITE_UTF_ENCODING = 'utf-8' #'utf16' #'utf-8' causes loss/corruption of certain characters\n",
    "HASHTAG_FREQUENCY_THRESHOLD = 100 # Minimum number of times a hashtag must be used to be included in the top hashtags\n",
    "NUMBER_TOP_HASHTAGS = 100 # Number of top hashtags to be included in the report/csv\n",
    "\n",
    "DATA_FOLDER_PATH = '..\\\\data\\\\'\n",
    "\n",
    "BACKUP_RAW_CSV_FILES_FOLDER_PATH = DATA_FOLDER_PATH + \"archive_backup\\\\\" # up Jupyter\\ Folder, into data\\archive_backup\\\n",
    "RAW_CSV_FILES_FOLDER_PATH = DATA_FOLDER_PATH + \"archive\\\\\" # up Jupyter\\ Folder, into data\\archive\\\n",
    "TOP_HASHTAGS_PER_DAY_FOLDER_PATH =  DATA_FOLDER_PATH + \"top_hashtags_per_day\\\\\" # up Jupyter\\ Folder, into data\\top_hashtags_per_day\\\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files/Data Setup\n",
    "* if you want to start from scratch, remove everything in the `data\\archive` folder and the \n",
    "    * the code will automatically copy the raw csvs from the `data\\archive_backup` folder to the `data\\archive` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder creation\n",
    "\n",
    "if not os.path.exists(BACKUP_RAW_CSV_FILES_FOLDER_PATH):\n",
    "    print(\"Unable to find backup folder: \" + BACKUP_RAW_CSV_FILES_FOLDER_PATH)\n",
    "    os.sys.exit(1)\n",
    "\n",
    "if not os.path.exists(RAW_CSV_FILES_FOLDER_PATH):\n",
    "    os.makedirs(RAW_CSV_FILES_FOLDER_PATH)\n",
    "    print(\"Created folder: \" + RAW_CSV_FILES_FOLDER_PATH)\n",
    "    \n",
    "if not os.path.exists(TOP_HASHTAGS_PER_DAY_FOLDER_PATH):\n",
    "    os.makedirs(TOP_HASHTAGS_PER_DAY_FOLDER_PATH)\n",
    "    print(\"Created folder: \" + TOP_HASHTAGS_PER_DAY_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment the opening \"\"\" in order to delete all files in both the `data\\archive` folder and the `data\\top_hashtags_per_day` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# https://stackoverflow.com/a/12526809\\ndef delete_folder_contents(dirpath):\\n    for filename in os.listdir(dirpath):\\n        filepath = os.path.join(dirpath, filename)\\n        try:\\n            shutil.rmtree(filepath)\\n        except OSError:\\n            os.remove(filepath)\\n\\ndelete_folder_contents(RAW_CSV_FILES_FOLDER_PATH)\\ndelete_folder_contents(TOP_HASHTAGS_PER_DAY_FOLDER_PATH)\\n           \\n#'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ONLY UNCOMMENT AND RUN THIS IF YOU WANT TO DELETE the contents of the archive\\ and top_hashtags_per_day\\ folder\n",
    "\n",
    "\"\"\"\n",
    "# https://stackoverflow.com/a/12526809\n",
    "def delete_folder_contents(dirpath):\n",
    "    for filename in os.listdir(dirpath):\n",
    "        filepath = os.path.join(dirpath, filename)\n",
    "        try:\n",
    "            shutil.rmtree(filepath)\n",
    "        except OSError:\n",
    "            os.remove(filepath)\n",
    "\n",
    "delete_folder_contents(RAW_CSV_FILES_FOLDER_PATH)\n",
    "delete_folder_contents(TOP_HASHTAGS_PER_DAY_FOLDER_PATH)\n",
    "           \n",
    "#\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy raw csvs from `data\\archive_backup` folder to `data\\archive` folder (if not already there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No raw csv files found in: ..\\data\\archive\\\n",
      "Copying files from: ..\\data\\archive_backup\\\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PERFORM_PROCESSING = False\n",
    "\n",
    "raw_csv_backup_filenames = [f for f in os.listdir(BACKUP_RAW_CSV_FILES_FOLDER_PATH) if f.endswith('.csv')]\n",
    "raw_csv_backup_filepaths = [os.path.join(BACKUP_RAW_CSV_FILES_FOLDER_PATH, f) for f in raw_csv_backup_filenames]\n",
    "raw_csv_filepaths = [os.path.join(RAW_CSV_FILES_FOLDER_PATH, f) for f in raw_csv_backup_filenames]\n",
    "\n",
    "# if both folders are empty, copy raw csvs from backup archive folder to raw archive folder\n",
    "if len(os.listdir(RAW_CSV_FILES_FOLDER_PATH)) == 0 and len(os.listdir(TOP_HASHTAGS_PER_DAY_FOLDER_PATH)) == 0:\n",
    "    print(\"No raw csv files found in: \" + RAW_CSV_FILES_FOLDER_PATH + \"\\nCopying files from: \" + BACKUP_RAW_CSV_FILES_FOLDER_PATH)\n",
    "\n",
    "    for backup_filepath, filepath in zip(raw_csv_backup_filepaths, raw_csv_filepaths):\n",
    "        shutil.copy(backup_filepath, filepath)\n",
    "        \n",
    "    PERFORM_PROCESSING = True\n",
    "    \n",
    "elif len(os.listdir(RAW_CSV_FILES_FOLDER_PATH)) > 0 and len(os.listdir(TOP_HASHTAGS_PER_DAY_FOLDER_PATH)) > 0:\n",
    "    PERFORM_PROCESSING = False\n",
    "    \n",
    "elif len(os.listdir(RAW_CSV_FILES_FOLDER_PATH)) > 0 and len(os.listdir(TOP_HASHTAGS_PER_DAY_FOLDER_PATH)) == 0:\n",
    "    PERFORM_PROCESSING = True\n",
    "\n",
    "print(PERFORM_PROCESSING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract all Date-Filenames pairings from the `data\\archive` folder using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{datetime.date(2022, 6, 13): ['0613_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 14): ['0614_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 15): ['0615_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 16): ['0616_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 17): ['0617_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 18): ['0618_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 19): ['0619_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 20): ['0620_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 21): ['0621_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 22): ['0622_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 23): ['0623_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 24): ['0624_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 25): ['0625_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 26): ['0626_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 27): ['0627_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 28): ['0628_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 29): ['0629_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 6, 30): ['0630_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 1): ['0701_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 2): ['0702_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 3): ['0703_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 4): ['0704_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 5): ['0705_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 6): ['0706_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 7): ['0707_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 8): ['0708_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 9): ['0709_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 10): ['0710_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 11): ['0711_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 12): ['0712_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 13): ['0713_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 14): ['0714_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 15): ['0715_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 16): ['0716_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 17): ['0717_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 18): ['0718_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 19): ['0719_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 20): ['0720_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 21): ['0721_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 22): ['0722_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 23): ['0723_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 24): ['0724_UkraineCombinedTweetsDeduped.csv'], datetime.date(2022, 7, 25): ['0725_UkraineCombinedTweetsDeduped.csv']}\n"
     ]
    }
   ],
   "source": [
    "# this is a guard clause to prevent the script from running if the output files are already in the archive\\ folder\n",
    "if not PERFORM_PROCESSING: \n",
    "    os.sys.exit(1) # exit with error code 1 if we don't need to perform processing   \n",
    "\n",
    "\n",
    "# extract the date from the file name, to aid in joining/spliting the parts automatically\n",
    "\"\"\" CSVs are in one of these formats:\n",
    "    UkraineCombinedTweetsDeduped20220227-131611.csv\n",
    "    UkraineCombinedTweetsDeduped_FEB27.csv\n",
    "    UkraineCombinedTweetsDeduped_FEB28_part1.csv\n",
    "    UkraineCombinedTweetsDeduped_MAR27_to_28.csv\n",
    "    UkraineCombinedTweetsDeduped_MAR30_REAL.csv\n",
    "    0401_0UkraineCombinedTweetsDeduped\n",
    "    0505_to_0507_UkraineCombinedTweetsDeduped.csv\n",
    "\"\"\" \n",
    "# regex for each of these cases, extract the month and day\n",
    "\n",
    "UKRAINE_BLURB=\"UkraineCombinedTweetsDeduped\"\n",
    "FIRST_CSV_HOUR_MIN_SEC = '131611'\n",
    "DATA_YEAR = 2022\n",
    "\n",
    "month_abbr_to_int = dict((v,k) for v,k in zip([m.lower() for m in calendar.month_abbr[1:]], range(1, 13)))\n",
    "\n",
    "regex1 = re.compile(r'^' + UKRAINE_BLURB + r'(?P<year>\\d{4})(?P<month>\\d{2})(?P<day>\\d{2})-' + FIRST_CSV_HOUR_MIN_SEC + '.csv$')\n",
    "regex2 = re.compile(r'^' + UKRAINE_BLURB + r'_(?P<month_abbr>\\w{3})(?P<day>\\d{2})\\.csv$')\n",
    "regex3 = re.compile(r'^' + UKRAINE_BLURB + r'_(?P<month_abbr>\\w{3})(?P<day>\\d{2})_part\\d\\.csv$')\n",
    "regex4 = re.compile(r'^' + UKRAINE_BLURB + r'_(?P<month_abbr1>\\w{3})(?P<day1>\\d{2})_to_(?:\\w{3})?(?P<day2>\\d{2})\\.csv$')\n",
    "regex4_2 = re.compile(r'^' + UKRAINE_BLURB + r'_(?P<month_abbr1>\\w{3})(?P<day1>\\d{2})_to_(?P<month_abbr2>\\w{3})(?P<day2>\\d{2})\\.csv$')\n",
    "regex5 = re.compile(r'^' + UKRAINE_BLURB + r'_(?P<month_abbr>\\w{3})(?P<day>\\d{2})_REAL\\.csv$')\n",
    "regex6 = re.compile(r'^(?P<month>\\d{2})(?P<day>\\d{2})_' + UKRAINE_BLURB + r'.csv')\n",
    "regex7 = re.compile(r'^(?P<month1>\\d{2})(?P<day1>\\d{2})_to_(?:\\d{2})?(?P<day2>\\d{2})_' + UKRAINE_BLURB + r'.csv')\n",
    "regex7_2 = re.compile(r'^(?P<month1>\\d{2})(?P<day1>\\d{2})_to_(?P<month2>\\d{2})(?P<day2>\\d{2})_' + UKRAINE_BLURB + r'.csv')\n",
    "\n",
    "def extract_date_range_from_filename(filename):\n",
    "    m1 = re.match(regex1, filename)\n",
    "    m2 = re.match(regex2, filename)\n",
    "    m3 = re.match(regex3, filename)\n",
    "    m4 = re.match(regex4, filename)\n",
    "    m5 = re.match(regex5, filename)\n",
    "    m6 = re.match(regex6, filename)\n",
    "    m7 = re.match(regex7, filename)\n",
    "    \n",
    "    if m1:\n",
    "        return [datetime.date(year=int(m1.group('year')), month=int(m1.group('month')), day=int(m1.group('day')))]\n",
    "    if m2:\n",
    "        return [datetime.date(year=DATA_YEAR, month=month_abbr_to_int[m2.group('month_abbr').lower()], day=int(m2.group('day')))]\n",
    "    if m3:\n",
    "        return [datetime.date(year=DATA_YEAR, month=month_abbr_to_int[m3.group('month_abbr').lower()], day=int(m3.group('day')))]\n",
    "    if m4:\n",
    "        sdate = datetime.date(year=DATA_YEAR, month=month_abbr_to_int[m4.group('month_abbr1').lower()], day=int(m4.group('day1')))\n",
    "        \n",
    "        m4_2 = re.match(regex4_2, filename) # this is the case where the 2nd month is not specified\n",
    "        if m4_2:\n",
    "            edate = datetime.date(year=DATA_YEAR, month=month_abbr_to_int[m4_2.group('month_abbr2').lower()], day=int(m4_2.group('day2')))\n",
    "        else:\n",
    "            edate = datetime.date(year=DATA_YEAR, month=month_abbr_to_int[m4.group('month_abbr1').lower()], day=int(m4.group('day2')))\n",
    "       \n",
    "        # https://stackoverflow.com/a/66595046\n",
    "        return [sdate+datetime.timedelta(days=x) for x in range(0,(edate-sdate).days+1)]\n",
    "    if m5:\n",
    "        return [datetime.date(year=DATA_YEAR, month=month_abbr_to_int[m5.group('month_abbr').lower()], day=int(m5.group('day')))]\n",
    "    if m6:\n",
    "        return [datetime.date(year=DATA_YEAR, month=int(m6.group('month')), day=int(m6.group('day')))]\n",
    "    if m7:\n",
    "        sdate = datetime.date(year=DATA_YEAR, month=int(m7.group('month1')), day=int(m7.group('day1')))\n",
    "        \n",
    "        m7_2 = re.match(regex7_2, filename) # this is for the case where the second month is not specified\n",
    "        \n",
    "        if m7_2:\n",
    "            edate = datetime.date(year=DATA_YEAR, month=int(m7_2.group('month2')), day=int(m7.group('day2'))) \n",
    "        else:\n",
    "            edate = datetime.date(year=DATA_YEAR, month=int(m7.group('month1')), day=int(m7.group('day2'))) \n",
    "            \n",
    "        # https://stackoverflow.com/a/66595046\n",
    "        return [sdate+datetime.timedelta(days=x) for x in range(0,(edate-sdate).days+1)]\n",
    "\n",
    "            \n",
    "############### MAIN SCRIPT STARTS HERE ###################\n",
    "\n",
    "# Get dictionary of dates to filenames\n",
    "date_to_csv_filenames = {}\n",
    "\n",
    "for filename in os.listdir(RAW_CSV_FILES_FOLDER_PATH):\n",
    "    if filename.endswith('.csv'):\n",
    "        date_range = extract_date_range_from_filename(filename)\n",
    "        if date_range is not None:\n",
    "            for date in date_range:\n",
    "                if date not in date_to_csv_filenames:\n",
    "                    date_to_csv_filenames[date] = []\n",
    "                date_to_csv_filenames[date].append(filename)\n",
    "\n",
    "print(date_to_csv_filenames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine/Split and Rename raw csv files as needed across each Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming files in ..\\data\\archive\\ to match format: 2022_07_26.csv\n"
     ]
    }
   ],
   "source": [
    "if len(os.listdir(RAW_CSV_FILES_FOLDER_PATH)) > 0 and len(os.listdir(TOP_HASHTAGS_PER_DAY_FOLDER_PATH)) == 0:\n",
    "    PERFORM_PROCESSING = True\n",
    "\n",
    "# this is a guard clause to prevent the script from running if the output files are already in the archive\\ folder\n",
    "if not PERFORM_PROCESSING: \n",
    "    os.sys.exit(1) # exit with error code 1 if we don't need to perform processing   \n",
    "\n",
    "def get_new_filename_from_date(date):\n",
    "    return f'{date.year:04d}_{date.month:02d}_{date.day:02d}.csv'\n",
    "\n",
    "def combine_files_and_remove_predecessors(date, old_filenames):\n",
    "    print('Combining files ' + str(old_filenames) + ' for date: ' + str(date))\n",
    "    \n",
    "    dfs_to_combine = []\n",
    "    \n",
    "    for old_filename in old_filenames:\n",
    "        dfs_to_combine.append(pd.read_csv(RAW_CSV_FILES_FOLDER_PATH + '/' + old_filename, low_memory=PD_READ_CSV_LOW_MEMORY_FLAG))\n",
    "    \n",
    "    df_combined = pd.concat(dfs_to_combine)\n",
    "        \n",
    "    df_combined.to_csv(RAW_CSV_FILES_FOLDER_PATH + '/' + get_new_filename_from_date(date), index=False)\n",
    "    \n",
    "    # remove old files\n",
    "    for old_filename in old_filenames:\n",
    "        filepath = os.path.join(RAW_CSV_FILES_FOLDER_PATH, old_filename)\n",
    "        print(\"Removing file: \" + old_filename)\n",
    "        try:\n",
    "            shutil.rmtree(filepath)\n",
    "        except OSError:\n",
    "            os.remove(filepath)\n",
    "\n",
    "\n",
    "# from https://ws-dl.blogspot.com/2019/08/2019-08-03-tweetedat-finding-tweet.html\n",
    "def get_date_from_tweetid(tid):\n",
    "    offset = 1288834974657 # UTC offset in milliseconds\n",
    "    tstamp = (tid >> 22) + offset\n",
    "    return datetime.datetime.utcfromtimestamp(tstamp/1000).date()\n",
    "\n",
    "\n",
    "def split_file_over_necessary_dates(filename, dates):\n",
    "    print(\"Spliting filename: \" + filename + \" over \" + str(dates)) \n",
    "    \n",
    "    df_to_split = pd.read_csv(RAW_CSV_FILES_FOLDER_PATH + '/' + filename, low_memory=PD_READ_CSV_LOW_MEMORY_FLAG)\n",
    "    df_to_split['date'] = df_to_split['tweetid'].map(get_date_from_tweetid)\n",
    "    \n",
    "    # Create a new dataframe for each date, by filtering the original dataframe by the date\n",
    "    for date in dates:\n",
    "        df_date = df_to_split[pd.to_datetime(df_to_split['date']).dt.date == date]\n",
    "        \n",
    "        df_date.to_csv(RAW_CSV_FILES_FOLDER_PATH + '/' + get_new_filename_from_date(date), index=False)\n",
    "        \n",
    "    # Remove the original file\n",
    "    filepath = os.path.join(RAW_CSV_FILES_FOLDER_PATH, filename)\n",
    "    print(\"Removing file: \" + filename)\n",
    "    try:\n",
    "        shutil.rmtree(filepath)\n",
    "    except OSError:\n",
    "        os.remove(filepath)\n",
    "   \n",
    "\n",
    "############### MAIN SCRIPT STARTS HERE ###################\n",
    "\n",
    "last_date = None\n",
    "last_filename = \"\"\n",
    "current_filename = \"\"\n",
    "num_dates_emcompassed_by_single_file = 1\n",
    "\n",
    "for date in date_to_csv_filenames:\n",
    "    current_filename = date_to_csv_filenames[date][0]\n",
    "    \n",
    "    if len(date_to_csv_filenames[date]) > 1:\n",
    "        \n",
    "        combine_files_and_remove_predecessors(date, date_to_csv_filenames[date])\n",
    "        \n",
    "    elif current_filename != last_filename and num_dates_emcompassed_by_single_file > 1:\n",
    "        \n",
    "        dates = list(reversed([last_date-datetime.timedelta(days=x) for x in range(0,num_dates_emcompassed_by_single_file)]))\n",
    "        split_file_over_necessary_dates(last_filename, dates)\n",
    "        \n",
    "        num_dates_emcompassed_by_single_file = 1\n",
    "\n",
    "    if last_filename == current_filename:\n",
    "        num_dates_emcompassed_by_single_file += 1    \n",
    "       \n",
    "    last_date = date \n",
    "    last_filename = current_filename\n",
    "\n",
    "# Case where the last date is part of a file that needs to be split\n",
    "if current_filename != last_filename and num_dates_emcompassed_by_single_file > 1:\n",
    "    dates = list(reversed([last_date-datetime.timedelta(days=x) for x in range(0,num_dates_emcompassed_by_single_file)]))\n",
    "    split_file_over_necessary_dates(last_filename, dates)\n",
    "    \n",
    "    \n",
    "# Rename all files to the format indicated by get_new_filename_from_date(date)\n",
    "print(f'Renaming files in {RAW_CSV_FILES_FOLDER_PATH} to match format: {get_new_filename_from_date(datetime.datetime.now().date())}') \n",
    "\n",
    "# matches filenames of the form: 2019_08_03.csv, as defined by the get_new_filename_from_date(date) function\n",
    "regex8 = re.compile(r'^(?P<year>\\d{4})_(?P<month>\\d{2})_(?P<day>\\d{2}).csv$') \n",
    "\n",
    "for filename in os.listdir(RAW_CSV_FILES_FOLDER_PATH):\n",
    "    if not re.match(regex8, filename): # if the filename does not match the regex, rename it\n",
    "        os.rename(RAW_CSV_FILES_FOLDER_PATH + '/' + filename,\n",
    "                  RAW_CSV_FILES_FOLDER_PATH + '/' + get_new_filename_from_date(extract_date_range_from_filename(filename)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Processing to the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote top 100 hashtags to file: 2022_06_13.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_14.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_15.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_16.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_17.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_18.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_19.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_20.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_21.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_22.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_23.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_24.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_25.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_26.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_27.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_28.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_29.csv\n",
      "Wrote top 100 hashtags to file: 2022_06_30.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_01.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_02.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_03.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_04.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_05.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_06.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_07.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_08.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_09.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_10.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_11.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_12.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_13.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_14.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_15.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_16.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_17.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_18.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_19.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_20.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_21.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_22.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_23.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_24.csv\n",
      "Wrote top 100 hashtags to file: 2022_07_25.csv\n"
     ]
    }
   ],
   "source": [
    "# this is a guard clause to prevent the script from running if the output files are already in the archive\\ folder\n",
    "if not PERFORM_PROCESSING: \n",
    "    os.sys.exit(1) # exit with error code 1 if we don't need to perform processing   \n",
    "\n",
    "def simplify_hashtags(htag_json):\n",
    "    htag_json = htag_json.replace('\\'','\\\"')\n",
    "    return [ht['text'].lower() for ht in json.loads(htag_json)]\n",
    "    \n",
    "    \n",
    "# from https://ws-dl.blogspot.com/2019/08/2019-08-03-tweetedat-finding-tweet.html\n",
    "def get_tweet_timestamp(tid):\n",
    "    offset = 1288834974657 # UTC offset in milliseconds\n",
    "    tstamp = (tid >> 22) + offset\n",
    "    return datetime.datetime.utcfromtimestamp(tstamp/1000)\n",
    "\n",
    "\n",
    "def clean_ukraine_conflict_twitter_dataframe(df):\n",
    "\n",
    "    # only keep English tweets with non-empty hashtags hashtags\n",
    "    df = df.loc[(df['language'].map(lambda d: d == 'en')) & (df['hashtags'].map(lambda d: d != '[]')), ['tweetid','hashtags']] \n",
    "\n",
    "    df['hashtags'] = df['hashtags'].map(simplify_hashtags)\n",
    "\n",
    "    df['tweet_timestamp'] = df['tweetid'].map(get_tweet_timestamp)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_top_n_hashtags_to_freq_dict(df, num_top_hashtags=NUMBER_TOP_HASHTAGS):\n",
    "\n",
    "    hashtag_to_freq = {}\n",
    "\n",
    "    # count hashtags\n",
    "    for ht_list in df['hashtags'].to_dict().values(): # key is index, value is list of hashtags\n",
    "        for ht in ht_list:\n",
    "            if ht not in hashtag_to_freq:\n",
    "                hashtag_to_freq[ht] = 1\n",
    "            else:\n",
    "                hashtag_to_freq[ht] += 1\n",
    "                \n",
    "    # remove hashtags below threshold (for faster sorting, smaller filesizes)\n",
    "    for ht in list(hashtag_to_freq.keys()):\n",
    "        if hashtag_to_freq[ht] < HASHTAG_FREQUENCY_THRESHOLD:\n",
    "            del hashtag_to_freq[ht]\n",
    "        \n",
    "    # sort hashtags by frequency\n",
    "    return dict(sorted(hashtag_to_freq.items(), key=lambda item: item[1], reverse=True)[0:num_top_hashtags])\n",
    "\n",
    "\n",
    "def write_top_n_hashtags_to_csv(hashtag_to_freq_dict, filename, field_names):\n",
    "    with open(os.path.join(TOP_HASHTAGS_PER_DAY_FOLDER_PATH, filename), 'w', encoding=CSV_WRITE_UTF_ENCODING) as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=field_names, lineterminator = '\\n')\n",
    "        writer.writeheader()\n",
    "        \n",
    "        list_of_fieldname_dicts = []\n",
    "        for ht, freq in hashtag_to_freq_dict.items():\n",
    "            list_of_fieldname_dicts.append({'hashtag': ht, 'frequency': freq})\n",
    "            \n",
    "        writer.writerows(list_of_fieldname_dicts)\n",
    "\n",
    "\n",
    "############### MAIN SCRIPT STARTS HERE ###################\n",
    "\n",
    "field_names = ['hashtag','frequency']\n",
    "\n",
    "# these files should be properly organized by date now\n",
    "for filename in os.listdir(RAW_CSV_FILES_FOLDER_PATH):\n",
    "    df = pd.read_csv(os.path.join(RAW_CSV_FILES_FOLDER_PATH, filename), low_memory=PD_READ_CSV_LOW_MEMORY_FLAG)\n",
    "    df = clean_ukraine_conflict_twitter_dataframe(df)\n",
    "    hashtag_to_freq_dict = get_top_n_hashtags_to_freq_dict(df, NUMBER_TOP_HASHTAGS)\n",
    "    \n",
    "    \n",
    "    write_top_n_hashtags_to_csv(hashtag_to_freq_dict, filename, field_names)\n",
    "    print(\"Wrote top \" + str(NUMBER_TOP_HASHTAGS) + \" hashtags to file: \" + filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4a48f9b014948d177db8b96258f87377bfd9aeb2f0be1739b178ffdde29ce0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
